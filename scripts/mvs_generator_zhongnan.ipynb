{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is for loading outputs from LIOSAM and ORBSLAM, and convert them to the file as the input of read_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "working_directory = \"/home/charles/Documents/zhongnan/fastlio-color/test-offline-color/test-new-extrinsic\"\n",
    "# working_directory = \"/data/BSD_data/Daiwu_sample\"\n",
    "\n",
    "config_file = os.path.join(working_directory, \"index.json\")\n",
    "path_to_raw_image_folder = os.path.join(working_directory, \"raw_images\")\n",
    "path_to_selected_image_folder = os.path.join(working_directory, \"selected_images\")\n",
    "\n",
    "path_to_vo_file =  os.path.join(working_directory, \"vo_interpolated_odom.txt\")\n",
    "path_to_keyframe_vo_file =  os.path.join(working_directory, \"vo_odom_keyframes.txt\")\n",
    "path_to_keyframe_vo_trans_file =  os.path.join(working_directory, \"vo_odom_keyframes_transformed.txt\")\n",
    "\n",
    "# path_to_undistort_img = os.path.join(working_directory, \"/undistort_images\")# \"../data/Daiwu_sample/undistort_images/\"\n",
    "\n",
    "pcd_file = os.path.join(working_directory, \"scans-clean-mls-clean.pcd\")# \"../data/Daiwu_sample/mesh.ply\"\n",
    "mesh_file = os.path.join(working_directory, \"scans_mesh.ply\")# \"../data/Daiwu_sample/mesh.ply\"\n",
    "\n",
    "# trajectory_file =  os.path.join(working_directory, \"vo_pano.txt\")#\"../data/Daiwu_sample/vo_pano.txt\"\n",
    "\n",
    "# mvs_pose_result =  os.path.join(working_directory, \"mvs_pose_result.txt\")#\"../data/Daiwu_sample/mvs_pose_result.txt\"\n",
    "\n",
    "mvs_frame_result = os.path.join(working_directory, \"mvs_frame_result.txt\")#\"../data/Daiwu_sample/mvs_frame_result.txt\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image and calibration file, then undistort fisheye images and save into \"path_to_undistort_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_calib(filename):\n",
    "    # Load the json file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Get the image resolution width and height\n",
    "    width = data['camera_para']['w']\n",
    "    height = data['camera_para']['h']\n",
    "    # Get the camera intrinsic data\n",
    "    fx = data['camera_para']['fx']\n",
    "    fy = data['camera_para']['fy']\n",
    "    cx = data['camera_para']['cx']\n",
    "    cy = data['camera_para']['cy']\n",
    "    k1 = data['camera_para']['k1']\n",
    "    k2 = data['camera_para']['k2']\n",
    "    k3 = data['camera_para']['k3']\n",
    "    k4 = data['camera_para']['k4']\n",
    "\n",
    "    t_LC = data[\"Tlc\"]\n",
    "    t_ci = data[\"Tci\"]\n",
    "\n",
    "    return width, height, fx, fy, cx, cy, k1, k2, k3, k4, t_LC, t_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to correct distortion for pinhole camera images\n",
    "def undistort_image(img, k_matrix, distortion_coeffs):\n",
    "    h, w = img.shape[:2]\n",
    "    # Obtain the optimal new camera matrix based on the free scaling parameter (alpha)\n",
    "    # Alpha=0 means all distorted points will be corrected but with some black pixels\n",
    "    # Alpha=1 retains all original pixels but will also keep some distortion\n",
    "    distortion_coeffs = np.array(distortion_coeffs)  # distortion coefficients\n",
    "    new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(k_matrix, distortion_coeffs, (w, h), 1, (w, h))\n",
    "\n",
    "    # Undistort the image\n",
    "    undistorted_img = cv2.undistort(img, k_matrix, distortion_coeffs, None, new_camera_matrix)\n",
    "\n",
    "    return undistorted_img, new_camera_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vo from vo_file, then select keyframes from vo_file and save them into vo_keyframe_file\n",
    "import math\n",
    "\n",
    "def select_keyframe_from_vo(vo_file, vo_keyframe_file, threshold=0.5):\n",
    "    # Read VO pano\n",
    "    with open(vo_file, 'r') as f:\n",
    "        vo_pano = f.readlines()\n",
    "\n",
    "    keyframes = []\n",
    "    last_position = None\n",
    "\n",
    "    for line in vo_pano:\n",
    "        data = line.strip().split()\n",
    "        time = data[0]\n",
    "        x, y, z = float(data[1]), float(data[2]), float(data[3])\n",
    "\n",
    "        if last_position is None:\n",
    "            keyframes.append(line)\n",
    "            last_position = (x, y, z)\n",
    "        else:\n",
    "            distance = math.sqrt((x - last_position[0])**2 + (y - last_position[1])**2 + (z - last_position[2])**2)\n",
    "            if distance >= threshold:\n",
    "                keyframes.append(line)\n",
    "                last_position = (x, y, z)\n",
    "\n",
    "    # Save keyframes into vo_keyframe_file\n",
    "    with open(vo_keyframe_file, 'w') as f:\n",
    "        f.writelines(keyframes)\n",
    "\n",
    "select_keyframe_from_vo(path_to_vo_file, path_to_keyframe_vo_file, threshold=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images from image folder, then read keyframe info from vo_pano_file, then select keyframe images from image_folder,and store into output_folder\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_images(raw_image_folder, vo_keyframe_file, camera_intrinsics, selected_image_folder):\n",
    "    # Camera intrinsics\n",
    "    fx, fy, cx, cy, k1, k2, p3, p4 = camera_intrinsics\n",
    "\n",
    "    # Create output folder if not exists\n",
    "    if os.path.exists(selected_image_folder):\n",
    "        shutil.rmtree(selected_image_folder)\n",
    "    os.mkdir(selected_image_folder)\n",
    "\n",
    "\n",
    "    # Read VO pano\n",
    "    with open(vo_keyframe_file, 'r') as f:\n",
    "        vo_pano = f.readlines()\n",
    "\n",
    "    # Extract timestamps\n",
    "    timestamps = [float(line.split()[0]) for line in vo_pano]\n",
    "\n",
    "    image_files = sorted(os.listdir(raw_image_folder))\n",
    "\n",
    "    # For each image in folder\n",
    "    cnt = 0\n",
    "    for idx, image_name in tqdm(enumerate(image_files), total=len(image_files), desc='Processing keyframes'):\n",
    "        # Extract timestamp from image name\n",
    "        timestamp = float(image_name.rsplit('.', 1)[0])\n",
    "        \n",
    "        # If timestamp is in vo_pano\n",
    "        if timestamp in timestamps:\n",
    "            # Load image\n",
    "            img = cv2.imread(os.path.join(raw_image_folder, image_name))\n",
    "            selected_image = os.path.join(selected_image_folder, str(f\"{timestamp:.6f}\") + \".jpg\")\n",
    "            \n",
    "            # undistorted_img = undistort_image(img, fx, fy, cx, cy, k1, k2, k3, k4)\n",
    "            k_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])  # Replace with your camera matrix values\n",
    "            distortion_coeffs = [k1, k2, p3, p4]  # Replace with your distortion coefficients\n",
    "            undistorted_img, new_intrinsic = undistort_image(img, k_matrix, distortion_coeffs)\n",
    "\n",
    "            # Save image\n",
    "            cv2.imwrite(selected_image, undistorted_img)   \n",
    "            cnt += 1\n",
    "\n",
    "    return new_intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Resolution - Width: 4096, Height: 3000\n",
      "Camera Intrinsic Data - fx: 4818.200388954926, fy: 4819.10345841615, cx: 2032.4178620390019, cy: 1535.1895959282901\n",
      "/home/charles/Documents/zhongnan/fastlio-color/test-offline-color/test-new-extrinsic/selected_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keyframes: 100%|██████████| 588/588 [00:05<00:00, 117.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New camera intrinsic matrix is: [[4.83111156e+03 0.00000000e+00 2.03085034e+03]\n",
      " [0.00000000e+00 4.82355224e+03 1.53477753e+03]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "width, height, fx, fy, cx, cy, k1, k2, k3, k4, t_LC, tci = load_calib(config_file)\n",
    "cam_intrinsic = [ fx, fy, cx, cy, k1, k2, k3, k4]\n",
    "print('Image Resolution - Width: {}, Height: {}'.format(width, height))\n",
    "print('Camera Intrinsic Data - fx: {}, fy: {}, cx: {}, cy: {}'.format(fx, fy, cx, cy))\n",
    "print(path_to_selected_image_folder)\n",
    "new_intrtinsic = process_images(path_to_raw_image_folder, \n",
    "                                path_to_keyframe_vo_file,\n",
    "                                cam_intrinsic,\n",
    "                                path_to_selected_image_folder)\n",
    "print(\"New camera intrinsic matrix is: {}\".format(new_intrtinsic))\n",
    "new_fx, new_fy, new_cx, new_cy = [new_intrtinsic[0,0],new_intrtinsic[1,1],new_intrtinsic[0,2],new_intrtinsic[1,2] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform lidar pose to camera pose \n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "def transform_lo_to_vo(path_to_keyframe_vo_file, path_to_keyframe_vo_trans_file, t_LC, t_ci):\n",
    "    t_LC = np.array(t_LC).reshape(4, 4)\n",
    "    t_CL = np.linalg.inv(t_LC)\n",
    "    t_ci = np.array(t_ci).reshape(4,4)\n",
    "    t_ic = np.linalg.inv(t_ci)\n",
    "\n",
    "    # print(t_LC)\n",
    "    # print(t_CL)\n",
    "    transformed_keyframes = []\n",
    "\n",
    "    # Read VO pano\n",
    "    with open(path_to_keyframe_vo_file, 'r') as f:\n",
    "        vo_pano = f.readlines()\n",
    "\n",
    "    for line in vo_pano:\n",
    "        data = line.strip().split()\n",
    "        time = data[0]\n",
    "        x, y, z = float(data[1]), float(data[2]), float(data[3])\n",
    "        qx, qy, qz, qw = [float(d) for d in data[4:]]\n",
    "        \n",
    "        # Convert LiDAR position and quaternion to pose matrix\n",
    "        # lid_quaternion = [qw, qx, qy, qz]\n",
    "        lid_quaternion = [ qx, qy, qz, qw]\n",
    "        lid_rotation = R.from_quat(lid_quaternion).as_matrix()\n",
    "        lid_pose = np.eye(4)\n",
    "        lid_pose[:3, :3] = lid_rotation\n",
    "        lid_pose[:3, 3] = [x, y, z]\n",
    "\n",
    "        # 构造绕Z轴旋转180度的静态变换矩阵\n",
    "        theta = np.pi  # 180度转换为弧度\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        static_trans = np.array([[c, -s, 0, 0],\n",
    "                                [s, c, 0, 0],\n",
    "                                [0, 0, 1, 0],\n",
    "                                [0, 0, 0, 1]])\n",
    "                                \n",
    "        \n",
    "        # Transform LiDAR pose to Camera pose\n",
    "        # cam_pose = np.dot(t_LC, lid_pose)\n",
    "        # cam_pose = np.dot(lid_pose, t_LC)\n",
    "        # cam_pose = np.dot(lid_pose, t_LC) \n",
    "        \n",
    "        cam_pose = lid_pose\n",
    "\n",
    "\n",
    "        # cam_pose = np.dot(np.dot(t_CL, lid_pose), static_trans)\n",
    "\n",
    "        # Extract camera position and rotation matrix\n",
    "        cam_position = cam_pose[:3, 3]\n",
    "        cam_rotation_matrix = cam_pose[:3, :3]\n",
    "\n",
    "        # Convert camera rotation matrix to quaternion\n",
    "        cam_quaternion = R.from_matrix(cam_rotation_matrix).as_quat() # (x,y,z,w)\n",
    "        # Format the transformed pose for saving\n",
    "        transformed_line = f\"{time} {cam_position[0]} {cam_position[1]} {cam_position[2]} {cam_quaternion[0]} {cam_quaternion[1]} {cam_quaternion[2]} {cam_quaternion[3]}\\n\"\n",
    "        transformed_keyframes.append(transformed_line)\n",
    "\n",
    "    # Save transformed keyframes into the specified file\n",
    "    with open(path_to_keyframe_vo_trans_file, 'w') as f:\n",
    "        f.writelines(transformed_keyframes)\n",
    "\n",
    "# transform_lo_to_vo(path_to_keyframe_vo_file, path_to_keyframe_vo_trans_file, t_LC, tci)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract point info from ply file\n",
    "For each point, calculate how many keyframe images observed this point, and record the id of these keyframe images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from loguru import logger as logger\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#  Visualization using Open3D\n",
    "def visualize_point_cloud_and_cameras(vertices, rotation_matrix, camera_pose ):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    camera = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.3)\n",
    "    world_origin = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.8)\n",
    "\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation_matrix\n",
    "    transformation_matrix[:3, 3] = np.array([camera_pose[0], camera_pose[1],camera_pose[2]])\n",
    "\n",
    "    camera.transform(transformation_matrix)\n",
    "    vis.add_geometry(camera)\n",
    "    vis.add_geometry(world_origin)\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from loguru import logger as logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "# default FOV: vertical_fov=84.8, horizontal_fov=98.4\n",
    "# vertical_fov=54.8, horizontal_fov=58.4\n",
    "def calculate_keyframes(mesh_file, trajectory_file, t_LC, mvs_pose_result_file, vertical_fov=54.8, horizontal_fov=58.4):\n",
    "    # 读取mesh.ply文件\n",
    "    # mesh = o3d.io.read_triangle_mesh(mesh_file)\n",
    "    mesh = o3d.io.read_point_cloud(mesh_file)\n",
    "\n",
    "    vertices = np.asarray(mesh.points)\n",
    "    logger.info(\"Vertices number in mesh.ply is {}\".format(len(vertices)))\n",
    "\n",
    "    # 读取视觉里程计轨迹\n",
    "    trajectories = np.loadtxt(trajectory_file)\n",
    "\n",
    "    # Read transformation matric from camera to lidar\n",
    "    # t_LC = np.array(t_LC).reshape(4, 4)\n",
    "\n",
    "    # # Extract rotation matrix and translation vector\n",
    "    # R = t_LC[:3, :3]\n",
    "    # t = t_LC[:3, 3]\n",
    "\n",
    "    # # Calculate inverse transformation matrix from camera to LiDAR coordinates\n",
    "    # Rt = np.transpose(R)\n",
    "    # t_inv = -np.dot(Rt, t)\n",
    "    # t_CL = np.eye(4)  # Initialize a 4x4 identity matrix\n",
    "    # t_CL[:3, :3] = Rt\n",
    "    # t_CL[:3, 3] = t_inv\n",
    "    \n",
    "    # 初始化结果\n",
    "    result = []\n",
    "    num_of_visible_point = 0\n",
    "\n",
    "    # 初始化一个字典来存储每个点的关键帧ID\n",
    "    point_to_keyframes = {i: [] for i in range(len(vertices))}\n",
    "\n",
    "    # 使用hidden point removal计算在该viewpoint位置处，可见的点云\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "    # transform the pcd object from lidar coord to camera coord by dotting the TF\n",
    "    # pcd.transform(t_LC)\n",
    "    # o3d.visualization.draw([pcd])\n",
    "\n",
    "    diameter = np.linalg.norm(np.asarray(pcd.get_max_bound()) - np.asarray(pcd.get_min_bound()))\n",
    "    radius = diameter * 100\n",
    "    logger.info(\" hidden point removal radius is {}\".format(radius))\n",
    "\n",
    "    # 在转换点之前，创建一个索引映射\n",
    "    index_map = np.arange(len(vertices))\n",
    "\n",
    "\n",
    "    # 为单行情况创建一个虚拟的外层列表，使其与多行情况的处理逻辑保持一致\n",
    "    if len(trajectories.shape) == 1:\n",
    "        trajectories = [trajectories]\n",
    "        \n",
    "    # camera_poses = []\n",
    "    # 遍历每个关键帧\n",
    "    for idx, trajectory in tqdm(enumerate(trajectories), total=len(trajectories), desc='Processing keyframes'): # 获取相机姿态\n",
    "        # timestamp = trajectory[0]\n",
    "        camera_position = trajectory[1:4]\n",
    "        # qx, qy, qz, qw = trajectory[4:]\n",
    "        qw ,qx, qy, qz= trajectory[4:]\n",
    "        # quaternion = [qw, qx, qy, qz]\n",
    "        quaternion = [ qx, qy, qz, qw]\n",
    "        # 将四元数转换为旋转矩阵\n",
    "        camera_rotation = R.from_quat(quaternion).as_matrix() # rotation matrix from frame 0 to\n",
    "        camera_pose = np.eye(4)\n",
    "        camera_pose[:3, :3] = camera_rotation\n",
    "        camera_pose[:3, 3] = camera_position\n",
    "        camera_pose_inv = np.linalg.inv(camera_pose)\n",
    " \n",
    "        # 使用hidden point removal计算在该viewpoint位置处，可见的点云\n",
    "        _, pt_map = pcd.hidden_point_removal(camera_position, radius)\n",
    "\n",
    "        # visible_points_pcd = pcd.select_by_index(pt_map)\n",
    "        # o3d.io.write_point_cloud(\"visible_points.ply\", visible_points_pcd)\n",
    "        # o3d.visualization.draw([visible_points_pcd])\n",
    "\n",
    "        visible_points = np.asarray(pcd.points)[pt_map]\n",
    "\n",
    "        # 更新索引映射\n",
    "        visible_indices = index_map[pt_map]\n",
    "\n",
    "        # 使用相机pose以及相机的视场角，计算在该相机的pose处，可见的点云\n",
    "        camera_z_direction = np.array([0,0,1])\n",
    "        # camera_y_direction = np.array([0,1,0]) \n",
    "\n",
    "        visible_points_in_cam = camera_pose_inv[:3, :3] @  visible_points.transpose() + camera_pose_inv[:3, 3].reshape((3,1))\n",
    "        visible_points_in_cam = visible_points_in_cam.transpose()\n",
    "\n",
    "        visible_point_ranges = np.linalg.norm(visible_points_in_cam, axis=1)\n",
    "        theta_z = np.arccos(np.dot(visible_points_in_cam, camera_z_direction) / visible_point_ranges)  # incident angle to camera frame\n",
    "        zenith_angle = np.arccos(visible_points_in_cam[:,1] / visible_point_ranges)\n",
    "        theta_z_in_deg = np.rad2deg(theta_z)\n",
    "        zenith_angle_in_deg = np.rad2deg(zenith_angle)\n",
    "        # logger.info(\"For idx {}, theta_z_in_deg is {}, zenith_angle_in_deg is {}\".format(idx, theta_z_in_deg, zenith_angle_in_deg))\n",
    "        # mask_fov = np.logical_and(zenith_angle < np.deg2rad(90 + vertical_fov/2), np.deg2rad(90 - vertical_fov/2) < zenith_angle, \\\n",
    "        #     theta_z < np.deg2rad(horizontal_fov / 2),  np.deg2rad(- horizontal_fov / 2) < theta_z )\n",
    "        mask_zenith = np.logical_and(np.deg2rad(90 - vertical_fov/2) < zenith_angle, zenith_angle < np.deg2rad(90 + vertical_fov/2))\n",
    "        mask_theta = np.logical_and(np.deg2rad(- horizontal_fov / 2) < theta_z, theta_z < np.deg2rad(horizontal_fov / 2))\n",
    "        mask_fov = np.logical_and(mask_zenith, mask_theta)\n",
    "\n",
    "        # 对该view里面可见的点，添加该keyframe为能看到此点的关键帧\n",
    "        visible_points_in_fov = visible_points[mask_fov]\n",
    "       \n",
    "        # 对该view里面可见的点，添加该keyframe为能看到此点的关键帧\n",
    "        for i, point in enumerate(visible_points_in_fov):\n",
    "            original_index = visible_indices[i]\n",
    "            point_to_keyframes[original_index].append(idx)\n",
    "\n",
    "        # if visualize\n",
    "        # visualize_point_cloud_and_cameras(visible_points, camera_rotation, camera_position)\n",
    "        # visualize_point_cloud_and_cameras(visible_points_in_fov, camera_rotation, camera_position)\n",
    "\n",
    "\n",
    "        # for point in visible_points:\n",
    "        #     point_to_keyframes[np.where((vertices == point).all(axis=1))[0][0]].append(idx)\n",
    "\n",
    "    # 计算结果\n",
    "    logger.info(\"point_to_keyframes.items lens is {}\".format(len(point_to_keyframes.items())))\n",
    "    for point, keyframes in point_to_keyframes.items():\n",
    "        num_of_keyframes = len(keyframes)\n",
    "        if num_of_keyframes > 0:\n",
    "            num_of_visible_point += 1\n",
    "            result.append(f\"{vertices[point][0]} {vertices[point][1]} {vertices[point][2]} {num_of_keyframes} {' '.join(map(str, keyframes))}\")\n",
    "\n",
    "    return vertices, result, num_of_visible_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keyframes: 100%|██████████| 27/27 [00:00<00:00, 10628.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.71083328e+09 -1.88346820e-01  8.62947200e-02  9.96650000e-04\n",
      "  6.99219150e-01 -7.14673180e-01  1.79808000e-03 -1.82100000e-02]\n",
      "[ 1.71083328e+09 -1.97910160e-01 -1.27014560e-01  4.73792230e-01\n",
      "  5.98587410e-01 -7.66460880e-01  1.46331000e-01 -1.81157580e-01]\n",
      "[ 1.71083329e+09 -3.95816590e-01 -5.88155010e-01  4.24732900e-01\n",
      "  6.03932710e-01 -7.66056570e-01  1.45930110e-01 -1.64702850e-01]\n",
      "[ 1.7108333e+09  5.1838530e-02 -8.0528458e-01  6.2191405e-01\n",
      "  5.2992508e-01 -8.4769562e-01 -1.6161050e-02 -1.8175880e-02]\n",
      "[ 1.7108333e+09  5.5620274e-01 -7.2938203e-01  6.4741174e-01\n",
      "  5.2538298e-01 -8.1699381e-01 -2.1056237e-01  1.1026026e-01]\n",
      "[ 1.71083330e+09  1.04930114e+00 -5.37403170e-01  6.70148700e-01\n",
      "  4.90965080e-01 -7.92224780e-01 -3.03791220e-01  1.97595770e-01]\n",
      "[ 1.71083331e+09  1.39951189e+00 -1.51828650e-01  6.33210510e-01\n",
      "  4.55545060e-01 -7.16607410e-01 -4.42014480e-01  2.89094640e-01]\n",
      "[ 1.71083331e+09  1.50027147e+00  3.44898030e-01  5.83153860e-01\n",
      "  4.27129360e-01 -5.85226010e-01 -5.33145120e-01  4.36837850e-01]\n",
      "[ 1.71083332e+09  1.68683502e+00  8.56510690e-01  5.86464250e-01\n",
      "  3.54341360e-01 -4.77861270e-01 -6.27115130e-01  5.02809530e-01]\n",
      "[ 1.71083332e+09  1.66988466e+00  1.37517560e+00  5.51305360e-01\n",
      "  2.62125850e-01 -3.81829900e-01 -6.89892090e-01  5.56367560e-01]\n",
      "[ 1.71083332e+09  1.31295200e+00  1.73188188e+00  5.27874410e-01\n",
      "  1.76629090e-01 -2.87590290e-01 -7.37000030e-01  5.85597940e-01]\n",
      "[ 1.71083333e+09  8.67182730e-01  1.95324907e+00  4.56990810e-01\n",
      "  1.06482890e-01 -1.72540560e-01 -7.71175290e-01  6.03473140e-01]\n",
      "[ 1.71083333e+09  3.41342650e-01  2.06262011e+00  4.42378370e-01\n",
      "  5.58644000e-03 -4.17370500e-02 -7.85315200e-01  6.17662400e-01]\n",
      "[ 1.71083334e+09 -1.73470270e-01  2.07443909e+00  4.41411500e-01\n",
      "  9.47169100e-02 -1.03537270e-01  7.84515240e-01 -6.04023650e-01]\n",
      "[ 1.71083334e+09 -6.15983090e-01  1.81545332e+00  4.06108380e-01\n",
      "  1.87184770e-01 -2.39875700e-01  7.41208410e-01 -5.98357420e-01]\n",
      "[ 1.71083334e+09 -9.37057590e-01  1.42450415e+00  3.67660160e-01\n",
      "  2.92152370e-01 -3.54984280e-01  6.63510090e-01 -5.90243610e-01]\n",
      "[ 1.71083335e+09 -1.12622954e+00  9.62922980e-01  3.10943960e-01\n",
      "  3.73392500e-01 -5.42399400e-01  5.83176690e-01 -4.75695160e-01]\n",
      "[ 1.71083335e+09 -1.29210698e+00  4.52879200e-01  4.01417060e-01\n",
      "  4.48177030e-01 -6.25330070e-01  4.90086930e-01 -4.09773650e-01]\n",
      "[ 1.71083336e+09 -1.19362649e+00 -4.29283600e-02  5.05509730e-01\n",
      "  5.07147210e-01 -7.06438720e-01  3.70746350e-01 -3.26026360e-01]\n",
      "[ 1.71083336e+09 -8.94074880e-01 -4.47038910e-01  4.22507480e-01\n",
      "  5.41990990e-01 -7.54656620e-01  2.67683650e-01 -2.55116860e-01]\n",
      "[ 1.71083336e+09 -4.81517300e-01 -7.53713770e-01  3.62481880e-01\n",
      "  5.67674770e-01 -7.96575430e-01  1.53996760e-01 -1.39635020e-01]\n",
      "[ 1.71083337e+09 -2.72012300e-01 -4.12850400e-01  6.80943730e-01\n",
      "  5.03983770e-01 -8.43110850e-01  1.31434510e-01 -1.33751320e-01]\n",
      "[ 1.71083337e+09 -1.78697420e-01 -1.02761600e-02  9.65496500e-01\n",
      "  3.05766330e-01 -9.30867290e-01  1.66795500e-01 -1.10328140e-01]\n",
      "[ 1.71083339e+09  1.77347540e-01  3.33750010e-01  1.09885065e+00\n",
      "  7.55186900e-02 -9.35740580e-01 -3.43156430e-01  3.04986000e-02]\n",
      "[ 1.71083339e+09  6.84119700e-01  4.63991020e-01  1.03071682e+00\n",
      "  1.65905460e-01 -7.48094940e-01 -6.23761710e-01  1.54112510e-01]\n",
      "[ 1.71083339e+09  9.96852160e-01  2.97513450e-01  6.60535530e-01\n",
      "  3.64344470e-01 -7.36325870e-01 -5.00882750e-01  2.72385380e-01]\n",
      "[ 1.71083340e+09  1.28660715e+00  1.74487170e-01  2.51977750e-01\n",
      "  5.32499050e-01 -6.17128660e-01 -4.13230520e-01  4.06001860e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "trajectories = np.loadtxt(path_to_keyframe_vo_file)\n",
    "# write MVS keyframe info\n",
    "MVS_result = []\n",
    "MVS_result.append(f\"MVS {width} {height}\")\n",
    "MVS_result.append(f\"{len(trajectories)}\")\n",
    "\n",
    "# 为单行情况创建一个虚拟的外层列表，使其与多行情况的处理逻辑保持一致\n",
    "if len(trajectories.shape) == 1:\n",
    "    trajectories = [trajectories]\n",
    "\n",
    "for idx, trajectory in tqdm(enumerate(trajectories), total=len(trajectories), desc='Processing keyframes'): # 获取相机姿态\n",
    "    print(trajectory)\n",
    "    timestamp = trajectory[0]\n",
    "    camera_position = trajectory[1:4]\n",
    "    quaternion= trajectory[4:]\n",
    "    # quaternion = [ qx, qy, qz,qw]\n",
    "\n",
    "    # 将四元数转换为旋转矩阵\n",
    "    camera_rotation = R.from_quat(quaternion).as_matrix() # rotation matrix from frame 0 to\n",
    "    camera_pose = np.eye(4)\n",
    "    camera_pose[:3, :3] = camera_rotation\n",
    "    camera_pose[:3, 3] = camera_position\n",
    "    camera_pose_inv = np.linalg.inv(camera_pose)\n",
    "\n",
    "    camera_odom_cw_quat = R.from_matrix(camera_pose_inv[:3, :3]).as_quat() # x,y,z,w\n",
    "    camera_odom_cw_pos = camera_pose_inv[:3, 3]\n",
    "    [qx, qy, qz, qw] = camera_odom_cw_quat\n",
    "\n",
    "\n",
    "    MVS_result.append(f\"{idx} {new_fx} {new_fy} {new_cx} {new_cy} {qw} {qx} {qy} {qz} {camera_position[0]} {camera_position[1]} {camera_position[2]} {timestamp:6f}\")\n",
    "\n",
    "    # MVS_result.append(f\"{idx} {new_fx} {new_fy} {new_cx} {new_cy} {qw} {qx} {qy} {qz} {camera_odom_cw_pos[0]} {camera_odom_cw_pos[1]} {camera_odom_cw_pos[2]} {timestamp:6f}\")\n",
    "\n",
    "with open(mvs_frame_result, 'w') as file:\n",
    "    # file.write(f\"{len(vertices)}\\n\")\n",
    "    file.write(\"\\n\".join(MVS_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'open3d.cpu.pybind.geometry.PointCloud' object has no attribute 'vertices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m transform_matrix_LC \u001b[38;5;241m=\u001b[39m t_LC\n\u001b[0;32m----> 2\u001b[0m vertices, result, num_of_visible_point \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_keyframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_keyframe_vo_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_matrix_LC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmvs_frame_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 写入结果到txt文件\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(mvs_frame_result, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mcalculate_keyframes\u001b[0;34m(mesh_file, trajectory_file, t_LC, mvs_pose_result_file, vertical_fov, horizontal_fov)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_keyframes\u001b[39m(mesh_file, trajectory_file, t_LC, mvs_pose_result_file, vertical_fov\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m54.8\u001b[39m, horizontal_fov\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m58.4\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 读取mesh.ply文件\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# mesh = o3d.io.read_triangle_mesh(mesh_file)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_point_cloud(mesh_file)\n\u001b[0;32m---> 13\u001b[0m     vertices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertices\u001b[49m)\n\u001b[1;32m     14\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVertices number in mesh.ply is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(vertices)))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 读取视觉里程计轨迹\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'open3d.cpu.pybind.geometry.PointCloud' object has no attribute 'vertices'"
     ]
    }
   ],
   "source": [
    "transform_matrix_LC = t_LC\n",
    "vertices, result, num_of_visible_point = calculate_keyframes(pcd_file, path_to_keyframe_vo_file, transform_matrix_LC, mvs_frame_result)\n",
    "\n",
    "# 写入结果到txt文件\n",
    "with open(mvs_frame_result, 'a') as file:\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f\"{num_of_visible_point}\\n\")\n",
    "    file.write(\"\\n\".join(result))\n",
    "\n",
    "print(f\"Results appended to {mvs_frame_result}\")\n",
    "print(f\"num_of_visible_point {num_of_visible_point}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
