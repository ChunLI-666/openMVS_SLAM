{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is for loading outputs from LIOSAM and ORBSLAM, and convert them to the file as the input of read_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"../data/openmvs/BSD_data/Daiwu_sample/index.json\"\n",
    "path_to_image_folder = \"../data/openmvs/BSD_data/Daiwu_sample/images/\"\n",
    "path_to_vo_file = \"../data/openmvs/BSD_data/Daiwu_sample/vo_pano.txt\"\n",
    "path_to_undistort_img =  \"../data/openmvs/BSD_data/Daiwu_sample/undistort_images/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_calib(filename):\n",
    "    # Load the json file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Get the image resolution width and height\n",
    "    width = data['camera_para']['w']\n",
    "    height = data['camera_para']['h']\n",
    "    # Get the camera intrinsic data\n",
    "    fx = data['camera_para']['fx']\n",
    "    fy = data['camera_para']['fy']\n",
    "    cx = data['camera_para']['cx']\n",
    "    cy = data['camera_para']['cy']\n",
    "    k1 = data['camera_para']['k1']\n",
    "    k2 = data['camera_para']['k2']\n",
    "    k3 = data['camera_para']['k3']\n",
    "    k4 = data['camera_para']['k4']\n",
    "\n",
    "    t_LC = data[\"Tlc\"]\n",
    "\n",
    "    return width, height, fx, fy, cx, cy, k1, k2, k3, k4, t_LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to correct distortion\n",
    "def undistort_image(img, fx, fy, cx, cy, k1, k2, p3, p4):\n",
    "    h, w = img.shape[:2]\n",
    "    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    D = np.array([k1, k2, p3, p4]) # distortion coefficients\n",
    "    map1, map2 = cv2.fisheye.initUndistortRectifyMap(K, D, np.eye(3), K, (w, h), cv2.CV_16SC2)\n",
    "    undistorted_img = cv2.remap(img, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "    return undistorted_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process images\n",
    "def process_images(image_folder, vo_pano_file, camera_intrinsics, output_folder):\n",
    "    # Camera intrinsics\n",
    "    fx, fy, cx, cy, k1, k2, k3, k4 = camera_intrinsics\n",
    "\n",
    "    # Create output folder if not exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Read VO pano\n",
    "    with open(vo_pano_file, 'r') as f:\n",
    "        vo_pano = f.readlines()\n",
    "\n",
    "    # Extract timestamps\n",
    "    timestamps = [float(line.split()[0]) for line in vo_pano]\n",
    "\n",
    "    # For each image in folder\n",
    "    for image_name in os.listdir(image_folder):\n",
    "        # Extract timestamp from image name\n",
    "        timestamp = float(image_name.rsplit('.', 1)[0])\n",
    "        \n",
    "        # If timestamp is in vo_pano\n",
    "        if timestamp in timestamps:\n",
    "            # Load image\n",
    "            img = cv2.imread(os.path.join(image_folder, image_name))\n",
    "\n",
    "            # Undistort image\n",
    "            undistorted_img = undistort_image(img, fx, fy, cx, cy, k1, k2, k3, k4)\n",
    "\n",
    "            # Save image\n",
    "            cv2.imwrite(os.path.join(output_folder, image_name), undistorted_img)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Resolution - Width: 3072, Height: 3072\n",
      "Camera Intrinsic Data - fx: 965.5805661476971, fy: 965.6323281356813, cx: 1558.7622938172478, cy: 1554.3208640745897\n"
     ]
    }
   ],
   "source": [
    "width, height, fx, fy, cx, cy, k1, k2, k3, k4, t_LC = load_calib(config_file)\n",
    "cam_intrinsic = [ fx, fy, cx, cy, k1, k2, k3, k4]\n",
    "print('Image Resolution - Width: {}, Height: {}'.format(width, height))\n",
    "print('Camera Intrinsic Data - fx: {}, fy: {}, cx: {}, cy: {}'.format(fx, fy, cx, cy))\n",
    "\n",
    "\n",
    "# process_images(path_to_image_folder, \n",
    "#                 path_to_vo_file,\n",
    "#                 cam_intrinsic,\n",
    "#                 path_to_undistort_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract point info from ply file\n",
    "For each point, calculate how many keyframe images observed this point, and record the id of these keyframe images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open3d as o3d\n",
    "\n",
    "# # Load the PLY file\n",
    "# pcd = o3d.io.read_point_cloud(mesh_file)\n",
    "\n",
    "# # Get the points\n",
    "# points = np.asarray(pcd.points)\n",
    "# print(f'Total of {points.size} points in the {mesh_file}')\n",
    "# # Print the coordinates\n",
    "# # for i, point in enumerate(points):\n",
    "#     # print(f'Point {i}: x={point[0]}, y={point[1]}, z={point[2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_file = \"../data/openmvs/BSD_data/Daiwu_sample/mesh.ply\"\n",
    "trajectory_file = \"../data/openmvs/BSD_data/Daiwu_sample/vo_pano.txt\"\n",
    "mvs_pose_result = \"../data/openmvs/BSD_data/Daiwu_sample/mvs_pose_result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from loguru import logger as logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_keyframes(mesh_file, trajectory_file, t_LC, mvs_pose_result_file, vertical_fov=110, horizontal_fov=180):\n",
    "    # 读取mesh.ply文件\n",
    "    mesh = o3d.io.read_triangle_mesh(mesh_file)\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    logger.info(\"Vertices number in mesh.ply is {}\".format(len(vertices)))\n",
    "\n",
    "    # 读取视觉里程计轨迹\n",
    "    trajectories = np.loadtxt(trajectory_file)\n",
    "\n",
    "    # Read transformation matric from camera to lidar\n",
    "    t_LC = np.array(t_LC).reshape(4, 4)\n",
    "    t_CL = np.linalg.inv(t_LC) \n",
    "\n",
    "    # 初始化结果\n",
    "    result = []\n",
    "    num_of_keyframe_number = 0\n",
    "\n",
    "    # 初始化一个字典来存储每个点的关键帧ID\n",
    "    point_to_keyframes = {i: [] for i in range(len(vertices))}\n",
    "\n",
    "    # 使用hidden point removal计算在该viewpoint位置处，可见的点云\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "    # transform the pcd object from lidar coord to camera coord by dotting the TF\n",
    "    pcd.transform(t_CL)\n",
    "    # o3d.visualization.draw([pcd])\n",
    "\n",
    "    # 遍历每个关键帧\n",
    "    for idx, trajectory in tqdm(enumerate(trajectories), total=len(trajectories), desc='Processing keyframes'): # 获取相机姿态\n",
    "        timestamp = trajectory[0]\n",
    "        camera_pose = trajectory[1:4]\n",
    "        qx, qy, qz, qw = trajectory[4:]\n",
    "        quaternion = [qw, qx, qy, qz]\n",
    "        # 将四元数转换为旋转矩阵\n",
    "        rotation_matrix = o3d.geometry.get_rotation_matrix_from_quaternion(quaternion)\n",
    "\n",
    "        # # 使用hidden point removal计算在该viewpoint位置处，可见的点云\n",
    "        # pcd = o3d.geometry.PointCloud()\n",
    "        # pcd.points = o3d.utility.Vector3dVector(vertices)\n",
    "        _, pt_map = pcd.hidden_point_removal(camera_pose, radius=1.0)\n",
    "        visible_points = np.asarray(pcd.points)[pt_map]\n",
    "\n",
    "        # 使用相机pose以及相机的视场角，计算在该相机的pose处，可见的点云\n",
    "        camera_z_direction = np.array([0,0,1])\n",
    "        # camera_y_direction = np.array([0,1,0]) \n",
    "\n",
    "        # visible_points_in_cam = rotation_matrix @ (t_CL[:3, :3] @ visible_points.transpose() + t_CL[:3, 3].reshape((3,1))) + np.asarray(camera_pose).reshape((3,1))\n",
    "        visible_points_in_cam = rotation_matrix @  visible_points.transpose() + np.asarray(camera_pose).reshape((3,1))\n",
    "        visible_points_in_cam = visible_points_in_cam.transpose()\n",
    "        norm_vector =  np.linalg.norm(visible_points_in_cam)\n",
    "        cos_angle_z = np.dot(visible_points_in_cam, camera_z_direction) / norm_vector\n",
    "        theta_z  = np.arccos(cos_angle_z)\n",
    "        # cos_angle_y = np.dot(visible_points_in_cam, camera_y_direction) / np.linalg.norm(visible_points_in_cam)\n",
    "        # theta_y  = np.arccos(cos_angle_y)\n",
    "        zenith_angle = np.arccos(visible_points_in_cam[:,1] / norm_vector)\n",
    "        # logger.info(\"For idx {}, theta_z is {}, theta_y is {}\".format(idx, np.rad2deg(theta_z), np.rad2deg(theta_y)))\n",
    "        # mask_fov = np.logical_and(zenith_angle < np.deg2rad(90 + vertical_fov/2), np.deg2rad(90 - vertical_fov/2) < zenith_angle, \\\n",
    "        #     theta_z < np.deg2rad(horizontal_fov / 2),  np.deg2rad(- horizontal_fov / 2) < theta_z )\n",
    "        mask_zenith = np.logical_and(np.deg2rad(90 - vertical_fov/2) < zenith_angle, zenith_angle < np.deg2rad(90 + vertical_fov/2))\n",
    "        mask_theta = np.logical_and(np.deg2rad(- horizontal_fov / 2) < theta_z, theta_z < np.deg2rad(horizontal_fov / 2))\n",
    "        mask_fov = np.logical_and(mask_zenith, mask_theta)\n",
    "\n",
    "\n",
    "        # 对该view里面可见的点，添加该keyframe为能看到此点的关键帧\n",
    "        visible_points_in_fov = visible_points[mask_fov]\n",
    "        for point in visible_points_in_fov:\n",
    "            point_to_keyframes[np.where((vertices == point).all(axis=1))[0][0]].append(idx)\n",
    "\n",
    "        # for point in visible_points:\n",
    "        #     point_to_keyframes[np.where((vertices == point).all(axis=1))[0][0]].append(idx)\n",
    "\n",
    "    # 计算结果\n",
    "    logger.info(\"point_to_keyframes.items lens is {}\".format(len(point_to_keyframes.items())))\n",
    "    for point, keyframes in point_to_keyframes.items():\n",
    "        num_of_keyframes = len(keyframes)\n",
    "        if num_of_keyframes > 0:\n",
    "            # num_of_keyframe_number += 1\n",
    "           result.append(f\"{vertices[point][0]} {vertices[point][1]} {vertices[point][2]} {num_of_keyframes} {' '.join(map(str, keyframes))}\")\n",
    "\n",
    "    # 写入结果到txt文件\n",
    "    with open(mvs_pose_result_file, 'w') as file:\n",
    "        file.write(f\"{len(vertices)}\\n\")\n",
    "        file.write(\"\\n\".join(result))\n",
    "\n",
    "    print(f\"Results written to result.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 09:16:16.275 | INFO     | __main__:calculate_keyframes:10 - Vertices number in mesh.ply is 9462482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D INFO] Window window_0 created.\n",
      "[Open3D INFO] EGL headless mode enabled.\n",
      "[Open3D INFO] ICE servers: [\"stun:stun.l.google.com:19302\", \"turn:user:password@34.69.27.100:3478\", \"turn:user:password@34.69.27.100:3478?transport=tcp\"]\n",
      "FEngine (64 bits) created at 0x42f2eb0 (threading is enabled)\n",
      "EGL(1.5)\n",
      "OpenGL(4.1)\n",
      "[Open3D INFO] Set WEBRTC_STUN_SERVER environment variable add a customized WebRTC STUN server.\n",
      "[Open3D INFO] WebRTC Jupyter handshake mode enabled.\n"
     ]
    }
   ],
   "source": [
    "transform_matrix_LC = t_LC\n",
    "\n",
    "calculate_keyframes(mesh_file,trajectory_file,transform_matrix_LC, mvs_pose_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
